{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sparkify project - Feature engineering \n",
    "In the previous notebook, an extensive data exploration has been done to identify the different information in the dataset and how the information spreads and varies across users. Some categorical variables could be extracted from parsing some of the fields such as State or Device/experience used to access the platform. \n",
    "The user registration date distribution has also shown a wide distribution across the users.\n",
    "In this notebook, I will combine the gender categorical feature, the behavioral data related to page visited, and teh session activity numbers.\n",
    "This step is preliminary to the modeling of churn prediction.\n",
    "Once the feature dataset is built, it will be used for teh modeling part in the next notenbook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import Window\n",
    "from pyspark.sql.functions import udf, concat, lit, col , explode, array, least, dense_rank\n",
    "from pyspark.sql.functions import isnan, when\n",
    "from pyspark.sql.functions import sum as Fsum \n",
    "from pyspark.sql.functions import min as Fmin\n",
    "from pyspark.sql.functions import max as Fmax\n",
    "from pyspark.sql.functions import unix_timestamp, to_timestamp, datediff\n",
    "from pyspark.sql.functions import avg, stddev , count\n",
    "from pyspark.sql.functions import asc, desc, log\n",
    "from pyspark.sql.types import IntegerType, StringType, DoubleType, LongType, BooleanType, TimestampType, DateType\n",
    "from pyspark.ml.feature import RegexTokenizer, VectorAssembler, MinMaxScaler\n",
    "from pyspark.ml.classification import GBTClassifier, RandomForestClassifier, LinearSVC, LogisticRegression, NaiveBayes\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator, BinaryClassificationEvaluator\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "from matplotlib import pyplot as plt \n",
    "import seaborn as sns \n",
    "import scipy.stats as sp\n",
    "from datetime import datetime as dt \n",
    "from datetime import timedelta as td\n",
    "import itertools\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a Spark session\n",
    "spark = SparkSession.builder \\\n",
    "        .master(\"local\") \\\n",
    "        .appName(\"Sparkify\") \\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility functions\n",
    "def load_df(filename):\n",
    "    df = spark.read.json(data_path)\n",
    "    df.persist()\n",
    "    return df \n",
    "\n",
    "def drop_null_userId(df):\n",
    "    df = df.filter(\"userId <> ''\")\n",
    "    return df \n",
    "\n",
    "# Creating device type\n",
    "def get_device_type(x):\n",
    "    if 'Macintosh' in x:\n",
    "        return 'Mac'\n",
    "    elif 'Windows' in x:\n",
    "        return 'Windows'\n",
    "    elif 'Linux' in x:\n",
    "        return 'Android'\n",
    "    elif 'iPhone' in x:\n",
    "        return 'iPhone'\n",
    "    elif 'iPad' in x:\n",
    "        return 'iPad'\n",
    "    else: \n",
    "        return 'Unknown'\n",
    "\n",
    "get_device_udf = udf(lambda x:get_device_type(x), StringType())\n",
    "\n",
    "def take_last_2_char(x):\n",
    "    return x[-2:]\n",
    "take_last_2_char_udf = udf(lambda x:take_last_2_char(x), StringType())\n",
    "\n",
    "def add_columns(df):\n",
    "    df = df.withColumn('device',get_device_udf(col('userAgent')))\n",
    "    df = df.withColumn('experience',when(col('device').isin(['iPhone','iPad','Android']), 'Mobile').otherwise('Desktop'))\n",
    "    df = df.withColumn('location_state', take_last_2_char_udf(col('location')))\n",
    "    df = df.withColumn('state_group',when(~col('location_state').isin(['CA','TX','PA','FL']),'others').otherwise(col('location_state')))\n",
    "    return df \n",
    "    \n",
    "\n",
    "def clean_df(df):\n",
    "    # drop rows where userId is null\n",
    "    df = drop_null_userId(df)\n",
    "    \n",
    "    # convert timestamps fields to datetime with timestamp\n",
    "    from_ts_to_date = udf(lambda x: dt.fromtimestamp(x / 1000.0), TimestampType())\n",
    "    df = df.withColumn('registration_date', from_ts_to_date(col('registration')))\n",
    "    df = df.withColumn('event_date', from_ts_to_date(col('ts')))\n",
    "    \n",
    "    # Adding minimum and maximum timestamp of activities per user\n",
    "    df = df.withColumn('min_event_date', Fmin(col('event_date')).over(Window.partitionBy('UserId')))\n",
    "    df = df.withColumn('max_event_date', Fmax(col('event_date')).over(Window.partitionBy('UserId')))\n",
    "    \n",
    "    # Getting the corrected registration date for users starting to use the platform before registration date.\n",
    "    df = df.withColumn('registration_date_cor', least(col('registration_date'),col('min_event_date')))\n",
    "\n",
    "    # Dropping users acquired after the start of the data collection window\n",
    "    data_min_date = df.select('min_event_date').agg(Fmin('min_event_date')).collect()[0][0]\n",
    "    df = df.where(col('registration_date_cor') <= data_min_date)\n",
    "    \n",
    "    return df \n",
    "\n",
    "def save_df_to_json(df, path):\n",
    "    df.write.json(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading and cleaning dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"mini_sparkify_event_data.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[artist: string, auth: string, firstName: string, gender: string, itemInSession: bigint, lastName: string, length: double, level: string, location: string, method: string, page: string, registration: bigint, sessionId: bigint, song: string, status: bigint, ts: bigint, userAgent: string, userId: string]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = load_df(data_path)\n",
    "df.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[artist: string, auth: string, firstName: string, gender: string, itemInSession: bigint, lastName: string, length: double, level: string, location: string, method: string, page: string, registration: bigint, sessionId: bigint, song: string, status: bigint, ts: bigint, userAgent: string, userId: string, registration_date: timestamp, event_date: timestamp, min_event_date: timestamp, max_event_date: timestamp, registration_date_cor: timestamp]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = clean_df(df)\n",
    "df.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[artist: string, auth: string, firstName: string, gender: string, itemInSession: bigint, lastName: string, length: double, level: string, location: string, method: string, page: string, registration: bigint, sessionId: bigint, song: string, status: bigint, ts: bigint, userAgent: string, userId: string, registration_date: timestamp, event_date: timestamp, min_event_date: timestamp, max_event_date: timestamp, registration_date_cor: timestamp, device: string, experience: string, location_state: string, state_group: string]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = add_columns(df)\n",
    "df.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "220"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.select('userId').dropDuplicates().count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List of features to build\n",
    "Let's try to predict churn with the following features aggregated at user level: \n",
    "- gender\n",
    "- total number of active days\n",
    "- tenure (or number of days of registration as of last day of the data collection window)\n",
    "- total number of sessions\n",
    "- total number of songs\n",
    "- total listening length\n",
    "- number of paid sessions\n",
    "- total length of paid sessions\n",
    "- number of free sessions\n",
    "- total length of free sessions\n",
    "- paid session share\n",
    "- features related to page usage : \n",
    "    - number of next song \n",
    "    - number of add to playlist\n",
    "    - number of Roll advert\n",
    "    - number of submit upgrades\n",
    "    - number of upgrades \n",
    "    - number of submit downgrades\n",
    "    - number of downgrades \n",
    "    - number of added friends\n",
    "    - number of errors\n",
    "    - number of help page visits \n",
    "    - number of settings page visit\n",
    "    - number of thumbs-up\n",
    "    - number of thumbs-down\n",
    "    - number of logins/logouts\n",
    "    - number of home page bisit \n",
    "    - number of about page visit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the user list\n",
    "features = df.select('userId').dropDuplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Categorical features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "gender = df.select('userId','gender')\\\n",
    "            .dropDuplicates()\\\n",
    "            .withColumn('gender_flag', when(col('gender')=='F',1).otherwise(0))\\\n",
    "            .drop('gender')\\\n",
    "            .withColumnRenamed('gender_flag','gender')\n",
    "features = features.join(gender,on=['userId'], how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numerical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding columns\n",
    "data_max_date = df.select('event_date').agg(Fmax('event_date')).collect()[0][0]\n",
    "df = df.withColumn('active_day', col(\"event_date\").cast(DateType()))\n",
    "df = df.withColumn('tenure',datediff(lit(data_max_date), col('registration_date_cor')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tenure \n",
    "tenure = df.select('userId','tenure').dropDuplicates()\n",
    "features = features.join(tenure,on=['userId'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of active days \n",
    "active_days = df.select('userId','active_day')\\\n",
    "                    .dropDuplicates()\\\n",
    "                    .groupby('userId')\\\n",
    "                    .count()\\\n",
    "                    .withColumnRenamed(\"count\", \"active_days_count\")\n",
    "features = features.join(active_days,on=['userId'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# total number of sessions\n",
    "session_count_per_user = df.select('userId','sessionId')\\\n",
    "                                .dropDuplicates()\\\n",
    "                                .groupby('userId')\\\n",
    "                                .count()\\\n",
    "                                .withColumnRenamed(\"count\", \"session_count\")\n",
    "features = features.join(session_count_per_user,on=['userId'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# total number of songs per user\n",
    "song_count_per_user = df.select('userId','active_day','song')\\\n",
    "                                .dropDuplicates()\\\n",
    "                                .groupby('userId')\\\n",
    "                                .count()\\\n",
    "                                .withColumnRenamed(\"count\", \"song_count\")\n",
    "features = features.join(song_count_per_user,on=['userId'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# total length of active sessions\n",
    "total_length = df.select('userId','length')\\\n",
    "                                .groupby('userId')\\\n",
    "                                .sum()\\\n",
    "                                .withColumnRenamed(\"sum(length)\", \"total_length\")\n",
    "features = features.join(total_length,on=['userId'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of paid sessions\n",
    "paid_sessions = df.select('userId', 'sessionId')\\\n",
    "                        .where(df.level == 'paid')\\\n",
    "                        .groupBy('userId')\\\n",
    "                        .count() \\\n",
    "                        .withColumnRenamed('count', 'paid_sessions')\n",
    "features = features.join(paid_sessions,on=['userId'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# length of paid sessions\n",
    "paid_sessions_length = df.select('userId', 'length')\\\n",
    "                        .where(df.level == 'paid')\\\n",
    "                        .groupBy('userId')\\\n",
    "                        .sum() \\\n",
    "                        .withColumnRenamed('sum(length)', 'paid_sessions_length')\n",
    "features = features.join(paid_sessions_length,on=['userId'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of free sessions\n",
    "free_sessions = df.select('userId', 'sessionId')\\\n",
    "                        .where(df.level == 'free')\\\n",
    "                        .groupBy('userId')\\\n",
    "                        .count() \\\n",
    "                        .withColumnRenamed('count', 'free_sessions')\n",
    "features = features.join(free_sessions,on=['userId'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# length of free sessions\n",
    "free_sessions_length = df.select('userId', 'length')\\\n",
    "                        .where(df.level == 'free')\\\n",
    "                        .groupBy('userId')\\\n",
    "                        .sum() \\\n",
    "                        .withColumnRenamed('sum(length)', 'free_sessions_length')\n",
    "features = features.join(free_sessions_length,on=['userId'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# paid_session share\n",
    "paid_session_share = paid_sessions.join(session_count_per_user, on=['userId'], how='left')\\\n",
    "                            .withColumn('paid_session_share',col('paid_sessions')/col('session_count'))\\\n",
    "                            .select('userId','paid_session_share')\n",
    "features = features.join(paid_session_share,on=['userId'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[userId: string, About: bigint, Add Friend: bigint, Add to Playlist: bigint, Downgrade: bigint, Error: bigint, Help: bigint, Home: bigint, Logout: bigint, NextSong: bigint, Roll Advert: bigint, Save Settings: bigint, Settings: bigint, Submit Downgrade: bigint, Submit Upgrade: bigint, Thumbs Down: bigint, Thumbs Up: bigint, Upgrade: bigint]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# page usage \n",
    "page_count = df.select(\"userId\", \"page\").groupBy(\"userId\").pivot(\"page\") \\\n",
    "    .count().drop(\"Cancel\", \"Cancellation Confirmation\").fillna(0)\n",
    "page_count.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# joining page count to feature dataset\n",
    "features = features.join(page_count, on=['userId'], how='left')\n",
    "features = features.na.fill(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating the churn flag\n",
    "df = df.withColumn('churn', when(df.page == 'Cancellation Confirmation', 1)\\\n",
    "                       .otherwise(0))\n",
    "df = df.withColumn('user_churn_flag', Fmax('churn').over(Window.partitionBy('UserId')))\n",
    "label = df.select('userId','user_churn_flag').dropDuplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding the churn label to the final table \n",
    "final_dataset = features.join(label,on=['userId'],how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the datframe for modeling \n",
    "save_df_to_json(final_dataset,'mini_sparkify_features.json')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
